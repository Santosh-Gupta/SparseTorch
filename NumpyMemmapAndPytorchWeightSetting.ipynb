{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NumpyMemmapAndPytorchWeightSetting.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santosh-Gupta/SparseTorch/blob/master/NumpyMemmapAndPytorchWeightSetting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGq_DcbkuBxT",
        "colab_type": "text"
      },
      "source": [
        "# This notebook will demonstrate how to switch values between a memory mapped numpy file and a pytorch model's weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_OnfsnOkYpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy\n",
        "import random \n",
        "import hashlib\n",
        "import os\n",
        "\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "from collections import deque\n",
        "numpy.random.seed(12345)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKWOha-7km4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define model architecture \n",
        "\n",
        "#Skip gram model architecture \n",
        "#Main thing to know is that it has two embedding matrixes, one for the context embedding\n",
        "#   another for the target embeddings \n",
        "class SkipGramModel(nn.Module):\n",
        "    \"\"\"Skip gram model of word2vec.\n",
        "    Attributes:\n",
        "        emb_size: Embedding size.\n",
        "        emb_dimention: Embedding dimention, typically from 50 to 500.\n",
        "        u_embedding: Embedding for center word.\n",
        "        v_embedding: Embedding for neibor words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emb_size, emb_dimension):\n",
        "        \"\"\"Initialize model parameters.\n",
        "        Apply for two embedding layers.\n",
        "        Initialize layer weight\n",
        "        Args:\n",
        "            emb_size: Embedding size.\n",
        "            emb_dimention: Embedding dimention, typically from 50 to 500.\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
        "        self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
        "        self.init_emb()\n",
        "\n",
        "    def init_emb(self):\n",
        "        \"\"\"Initialize embedding weight like word2vec.\n",
        "        The u_embedding is a uniform distribution in [-0.5/em_size, 0.5/emb_size], and the elements of v_embedding are zeroes.\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        initrange = 0.5 / self.emb_dimension\n",
        "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
        "        # self.v_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        \"\"\"Forward process.\n",
        "        As pytorch designed, all variables must be batch format, so all input of this method is a list of word id.\n",
        "        Args:\n",
        "            pos_u: list of center word ids for positive word pairs.\n",
        "            pos_v: list of neibor word ids for positive word pairs.\n",
        "            neg_u: list of center word ids for negative word pairs.\n",
        "            neg_v: list of neibor word ids for negative word pairs.\n",
        "        Returns:\n",
        "            Loss of this process, a pytorch variable.\n",
        "        \"\"\"\n",
        "        emb_u = self.u_embeddings(pos_u)\n",
        "        emb_v = self.v_embeddings(pos_v)\n",
        "        score = torch.mul(emb_u, emb_v).squeeze()\n",
        "        score = torch.sum(score, dim=1)\n",
        "        score = F.logsigmoid(score)\n",
        "        neg_emb_v = self.v_embeddings(neg_v)\n",
        "        neg_score = torch.bmm(neg_emb_v, emb_u.unsqueeze(2)).squeeze()\n",
        "        neg_score = F.logsigmoid(-1 * neg_score)\n",
        "        return -1 * (torch.sum(score)+torch.sum(neg_score))\n",
        "\n",
        "    def save_embedding(self, id2word, file_name, use_cuda):\n",
        "        \"\"\"Save all embeddings to file.\n",
        "        As this class only record word id, so the map from id to word has to be transfered from outside.\n",
        "        Args:\n",
        "            id2word: map from word id to word.\n",
        "            file_name: file name.\n",
        "        Returns:\n",
        "            None.\n",
        "        \"\"\"\n",
        "        if use_cuda:\n",
        "            embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
        "        else:\n",
        "            embedding = self.u_embeddings.weight.data.numpy()\n",
        "        fout = open(file_name, 'w')\n",
        "        fout.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
        "        for wid, w in id2word.items():\n",
        "            e = embedding[wid]\n",
        "            e = ' '.join(map(lambda x: str(x), e))\n",
        "            fout.write('%s %s\\n' % (w, e))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_i0HCNwk_Ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create instance of model with 20 embeddings, and embedding dimension of 7 \n",
        "\n",
        "skip_gram_model = SkipGramModel( 20, 7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-xrq0h0k_Ha",
        "colab_type": "code",
        "outputId": "a2dc04e4-d953-4e11-a2bb-9ff7f9db68e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "skip_gram_model.parameters()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7f4c7cbbaca8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77zHPX6HlJYC",
        "colab_type": "code",
        "outputId": "86422498-5671-452e-c2f1-4ebd53ac6733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#In this demonstration we'll only be looking at the u_embeddings embeddings \n",
        "\n",
        "print(skip_gram_model.u_embeddings)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding(20, 7, sparse=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0jtOGf-lLja",
        "colab_type": "code",
        "outputId": "5eb35626-85bb-4e09-9a5d-4fbd3daaf684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "source": [
        "#Look at all the weights \n",
        "\n",
        "print(skip_gram_model.u_embeddings.weight)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0578,  0.0374, -0.0301, -0.0211,  0.0535, -0.0149,  0.0655],\n",
            "        [ 0.0596, -0.0679,  0.0167, -0.0334, -0.0076, -0.0709, -0.0613],\n",
            "        [ 0.0532,  0.0502,  0.0086, -0.0567,  0.0010, -0.0557, -0.0175],\n",
            "        [-0.0375, -0.0149, -0.0670,  0.0542, -0.0169,  0.0195,  0.0521],\n",
            "        [-0.0495,  0.0456, -0.0076,  0.0372,  0.0388,  0.0066, -0.0177],\n",
            "        [ 0.0657, -0.0443, -0.0228,  0.0278,  0.0139,  0.0519, -0.0108],\n",
            "        [ 0.0188,  0.0707,  0.0588, -0.0697, -0.0407, -0.0531,  0.0627],\n",
            "        [ 0.0064,  0.0477, -0.0046,  0.0413,  0.0040,  0.0397,  0.0645],\n",
            "        [-0.0233, -0.0188, -0.0020, -0.0410,  0.0633,  0.0657,  0.0566],\n",
            "        [ 0.0138, -0.0520,  0.0215, -0.0156,  0.0462,  0.0063,  0.0059],\n",
            "        [ 0.0074,  0.0436,  0.0696,  0.0507,  0.0141,  0.0652, -0.0039],\n",
            "        [ 0.0565, -0.0014, -0.0394, -0.0315, -0.0079,  0.0554, -0.0372],\n",
            "        [-0.0019,  0.0710,  0.0073, -0.0193,  0.0506, -0.0224,  0.0446],\n",
            "        [-0.0330,  0.0274,  0.0635,  0.0165, -0.0063,  0.0292, -0.0686],\n",
            "        [ 0.0672, -0.0376, -0.0229,  0.0018,  0.0120,  0.0270, -0.0521],\n",
            "        [-0.0373,  0.0118,  0.0441,  0.0003,  0.0531,  0.0094,  0.0049],\n",
            "        [ 0.0149,  0.0573,  0.0176,  0.0098, -0.0486,  0.0448,  0.0070],\n",
            "        [-0.0025,  0.0180,  0.0040, -0.0662,  0.0052,  0.0132, -0.0233],\n",
            "        [-0.0059, -0.0458,  0.0475, -0.0565,  0.0631,  0.0394,  0.0558],\n",
            "        [ 0.0214,  0.0704,  0.0564, -0.0658, -0.0416,  0.0234, -0.0371]],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mevcb6zBlNhr",
        "colab_type": "code",
        "outputId": "bfe64321-d64c-47ef-dbc1-d4992c5331c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "# Look at the Pytorch tensor of the weights \n",
        "\n",
        "print(skip_gram_model.u_embeddings.weight.data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0578,  0.0374, -0.0301, -0.0211,  0.0535, -0.0149,  0.0655],\n",
            "        [ 0.0596, -0.0679,  0.0167, -0.0334, -0.0076, -0.0709, -0.0613],\n",
            "        [ 0.0532,  0.0502,  0.0086, -0.0567,  0.0010, -0.0557, -0.0175],\n",
            "        [-0.0375, -0.0149, -0.0670,  0.0542, -0.0169,  0.0195,  0.0521],\n",
            "        [-0.0495,  0.0456, -0.0076,  0.0372,  0.0388,  0.0066, -0.0177],\n",
            "        [ 0.0657, -0.0443, -0.0228,  0.0278,  0.0139,  0.0519, -0.0108],\n",
            "        [ 0.0188,  0.0707,  0.0588, -0.0697, -0.0407, -0.0531,  0.0627],\n",
            "        [ 0.0064,  0.0477, -0.0046,  0.0413,  0.0040,  0.0397,  0.0645],\n",
            "        [-0.0233, -0.0188, -0.0020, -0.0410,  0.0633,  0.0657,  0.0566],\n",
            "        [ 0.0138, -0.0520,  0.0215, -0.0156,  0.0462,  0.0063,  0.0059],\n",
            "        [ 0.0074,  0.0436,  0.0696,  0.0507,  0.0141,  0.0652, -0.0039],\n",
            "        [ 0.0565, -0.0014, -0.0394, -0.0315, -0.0079,  0.0554, -0.0372],\n",
            "        [-0.0019,  0.0710,  0.0073, -0.0193,  0.0506, -0.0224,  0.0446],\n",
            "        [-0.0330,  0.0274,  0.0635,  0.0165, -0.0063,  0.0292, -0.0686],\n",
            "        [ 0.0672, -0.0376, -0.0229,  0.0018,  0.0120,  0.0270, -0.0521],\n",
            "        [-0.0373,  0.0118,  0.0441,  0.0003,  0.0531,  0.0094,  0.0049],\n",
            "        [ 0.0149,  0.0573,  0.0176,  0.0098, -0.0486,  0.0448,  0.0070],\n",
            "        [-0.0025,  0.0180,  0.0040, -0.0662,  0.0052,  0.0132, -0.0233],\n",
            "        [-0.0059, -0.0458,  0.0475, -0.0565,  0.0631,  0.0394,  0.0558],\n",
            "        [ 0.0214,  0.0704,  0.0564, -0.0658, -0.0416,  0.0234, -0.0371]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UArqTNwmlRMR",
        "colab_type": "code",
        "outputId": "ab903ee8-d7f0-4e2a-a6cd-557291754e2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Look at only one embedding \n",
        "\n",
        "print(skip_gram_model.u_embeddings.weight.data[16])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.0149,  0.0573,  0.0176,  0.0098, -0.0486,  0.0448,  0.0070])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S54YZ3DBn3t5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Advice from https://stackoverflow.com/questions/49768306/pytorch-tensor-to-numpy-array\n",
        "# convert Pytorch tensor to numpy \n",
        "\n",
        "numpyArrayFromPytorch = skip_gram_model.u_embeddings.weight.data[16].cpu().detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGFd0dB2oPaH",
        "colab_type": "code",
        "outputId": "01ae9452-71aa-4ffc-83b5-a9560fefa297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Numpy variable version of embedding 16\n",
        "\n",
        "print(numpyArrayFromPytorch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.01489079  0.05732322  0.01762484  0.00976089 -0.04861376  0.04481397\n",
            "  0.00704634]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACEnrQyzvAU4",
        "colab_type": "text"
      },
      "source": [
        "# Create Memory mapped numpy file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiFe99vxlaL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 20 embeddings of embedding size 7 \n",
        "\n",
        "varMapped = np.memmap('test.mymemmap', dtype='float32', mode='w+', shape=(20 , 7))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vJWu845qUQ5",
        "colab_type": "code",
        "outputId": "fe60fe6f-841d-476c-98c0-477e35180001",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#check embedding 16\n",
        "\n",
        "varMapped[16]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "memmap([0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpDRfI5xqgcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set memory mapped variable from the pytorch model variable \n",
        "\n",
        "varMapped[16] = numpyArrayFromPytorch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4PwTWZiqrX2",
        "colab_type": "code",
        "outputId": "ee33642b-3993-4f66-b342-1f2255e507d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#  We have succesfully stored the Pytorch embedding  into the memory mapped file! \n",
        "\n",
        "varMapped[16]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "memmap([ 0.01489079,  0.05732322,  0.01762484,  0.00976089, -0.04861376,\n",
              "         0.04481397,  0.00704634], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmrJZZEewcsD",
        "colab_type": "text"
      },
      "source": [
        "# Now lets store an embedding from the memory mapped file into the Pytorch model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DwvqvhcrCgl",
        "colab_type": "code",
        "outputId": "9f9f9413-63e3-4b99-dabb-e465eb17ac70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Create random embedding to store into memory mapped file \n",
        "\n",
        "sampl = np.random.uniform(low=-1.0, high=1.0, size=(7,)).astype('f')\n",
        "print(sampl) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.8592322  -0.3672489  -0.6321624  -0.59087944  0.13545007  0.1910894\n",
            "  0.92902905]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ_NtuhyrMUW",
        "colab_type": "code",
        "outputId": "9d4dffcb-5eb7-4383-f5fa-94dfd124094b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "varMapped[9] "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "memmap([0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp7jeNyPq5DX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#set memory mappened embedding to random embedding\n",
        "\n",
        "varMapped[9] = sampl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN4dsw3erPa1",
        "colab_type": "code",
        "outputId": "bd33785f-9c2e-48e7-f271-44855ba40aec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "varMapped[9] "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "memmap([ 0.8592322 , -0.3672489 , -0.6321624 , -0.59087944,  0.13545007,\n",
              "         0.1910894 ,  0.92902905], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWs46hkIrTvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set Pytorch model embedding from the memory mapped file \n",
        "#Set up from https://discuss.pytorch.org/t/layer-weight-vs-weight-data/24271/2?u=santosh-gupta\n",
        "# need to temporary turn of gradient operations \n",
        "\n",
        "with torch.no_grad():\n",
        "    skip_gram_model.u_embeddings.weight.data[9] = torch.from_numpy( varMapped[9] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoqsWYqQtT90",
        "colab_type": "code",
        "outputId": "a2ab2d7d-36cf-4c4b-b7e4-d9c7a08bf961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Successfully set Pytorch model embedding from memory mapped file! \n",
        "\n",
        "skip_gram_model.u_embeddings.weight.data[9]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.8592, -0.3672, -0.6322, -0.5909,  0.1355,  0.1911,  0.9290])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}