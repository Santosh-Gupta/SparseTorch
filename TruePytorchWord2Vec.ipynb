{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TruePytorchWord2Vec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santosh-Gupta/SparseTorch/blob/master/TruePytorchWord2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZhhRX3R-lMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy\n",
        "import random \n",
        "import hashlib\n",
        "\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "from collections import deque\n",
        "numpy.random.seed(12345)\n",
        "\n",
        "use_cuda = True\n",
        "# if use_cuda and torch.cuda.is_available():\n",
        "#     net.cuda()\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import collections\n",
        "import hashlib\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from tempfile import gettempdir\n",
        "import zipfile\n",
        "\n",
        "data_index = 0\n",
        "vocabulary_size = 50000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRR5s_cHCuWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from tempfile import gettempdir\n",
        "import urllib.request\n",
        "from six.moves import xrange\n",
        "import zipfile\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0EIo7x6-f2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _hash_file(fpath):\n",
        "    hasher = hashlib.sha256()\n",
        "    with open(fpath, 'rb') as fpath_file:\n",
        "        for chunk in iter(lambda: fpath_file.read(65535), b''):\n",
        "            hasher.update(chunk)\n",
        "    return hasher.hexdigest()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48TRjGXrButV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # Create the directory for TensorBoard variables if there is not.\n",
        "  if not os.path.exists('log_dir'):\n",
        "    os.makedirs('log_dir')\n",
        "\n",
        "  # Step 1: Download the data.\n",
        "  # Note: Source website does not support HTTPS right now.\n",
        "  url = 'http://mattmahoney.net/dc/'\n",
        "\n",
        "  # pylint: disable=redefined-outer-name\n",
        "  def maybe_download(filename, expected_bytes, sha256=None):\n",
        "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "    local_filename = os.path.join(gettempdir(), filename)\n",
        "    if not os.path.exists(local_filename):\n",
        "      local_filename, _ = urllib.request.urlretrieve(url + filename,\n",
        "                                                     local_filename)\n",
        "    statinfo = os.stat(local_filename)\n",
        "\n",
        "    if sha256 and _hash_file(local_filename) != sha256:\n",
        "      raise Exception('Failed to verify ' + local_filename + ' due to hash '\n",
        "                      'mismatch. Can you get to it with a browser?')\n",
        "\n",
        "    if statinfo.st_size == expected_bytes:\n",
        "      print('Found and verified', filename)\n",
        "    else:\n",
        "      print(statinfo.st_size)\n",
        "      raise Exception('Failed to verify ' + local_filename +\n",
        "                      '. Can you get to it with a browser?')\n",
        "    return local_filename"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh71nomcb4sN",
        "colab_type": "code",
        "outputId": "5f5160c9-6c2f-4587-c387-eac188b0d71b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "filename = maybe_download(\n",
        "    'text8.zip',\n",
        "    31344016,\n",
        "    sha256='a6640522afe85d1963ad56c05b0ede0a0c000dddc9671758a6cc09b7a38e5232')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found and verified text8.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D8tqvBtDOUT",
        "colab_type": "code",
        "outputId": "54de3aaa-2fc4-4890-e89b-d1f2500b9b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        " # Read the data into a list of strings.\n",
        "def read_data(filename):\n",
        "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
        "    with zipfile.ZipFile(filename) as f:\n",
        "      data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
        "    return data\n",
        "\n",
        "vocabulary = read_data(filename)\n",
        "print('Data size', len(vocabulary))\n",
        "\n",
        "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
        "vocabulary_size = 50000"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size 17005207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHhzt3Yv-wiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dataset(words, n_words):\n",
        "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
        "    count = [['UNK', -1]]\n",
        "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
        "    dictionary = {}\n",
        "    for word, _ in count:\n",
        "      dictionary[word] = len(dictionary)\n",
        "    data = []\n",
        "    unk_count = 0\n",
        "    for word in words:\n",
        "      index = dictionary.get(word, 0)\n",
        "      if index == 0:  # dictionary['UNK']\n",
        "        unk_count += 1\n",
        "      data.append(index)\n",
        "    count[0][1] = unk_count\n",
        "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return data, count, dictionary, reversed_dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtPpicd6AbT7",
        "colab_type": "code",
        "outputId": "0009395d-2274-43c3-a0f9-232a5a3d215e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "  # Filling 4 global variables:\n",
        "  # data - list of codes (integers from 0 to vocabulary_size-1).\n",
        "  #   This is the original text but words are replaced by their codes\n",
        "  # count - map of words(strings) to count of occurrences\n",
        "  # dictionary - map of words(strings) to their codes(integers)\n",
        "  # reverse_dictionary - maps codes(integers) to words(strings)\n",
        "data, count, unused_dictionary, reverse_dictionary = build_dataset(\n",
        "    vocabulary, vocabulary_size)\n",
        "del vocabulary  # Hint to reduce memory.\n",
        "print('Most common words (+UNK)', count[:5])\n",
        "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
            "Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAXWfSRuAoq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\t  # Step 3: Function to generate a training batch for the skip-gram model.\n",
        "def generate_batch(batch_size, num_skips, skip_window, negRate):\n",
        "    global data_index\n",
        "    assert batch_size % num_skips == 0\n",
        "    assert num_skips <= 2 * skip_window\n",
        "\n",
        "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    negs = np.ndarray(shape=(batch_size, negRate), dtype=np.int32)\n",
        "\n",
        "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
        "    buffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builtin\n",
        "    if data_index + span > len(data):\n",
        "      data_index = 0\n",
        "    buffer.extend(data[data_index:data_index + span])\n",
        "    data_index += span\n",
        "    for i in range(batch_size // num_skips):\n",
        "      context_words = [w for w in range(span) if w != skip_window]\n",
        "      words_to_use = random.sample(context_words, num_skips)\n",
        "      for j, context_word in enumerate(words_to_use):\n",
        "        batch[i * num_skips + j] = buffer[skip_window]\n",
        "        labels[i * num_skips + j, 0] = buffer[context_word]\n",
        "        excludez = np.concatenate([  labels[i * num_skips + j] , [batch[i * num_skips + j] ]])\n",
        "        ij=0\n",
        "        while ij < negRate:\n",
        "            numCan = int(random.random() * (vocabulary_size/10))\n",
        "            if numCan in excludez or numCan in negs[ i * num_skips + j ]:\n",
        "                continue\n",
        "            negs[i * num_skips + j, ij] = numCan\n",
        "            ij = ij + 1\n",
        "\n",
        "      if data_index == len(data):\n",
        "        buffer.extend(data[0:span])\n",
        "        data_index = span\n",
        "      else:\n",
        "        buffer.append(data[data_index])\n",
        "        data_index += 1\n",
        "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
        "    data_index = (data_index + len(data) - span) % len(data)\n",
        "    return batch, labels, negs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mezgZTRE_gxn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "a396e5f9-09f0-4bef-d85b-8d14615ad653"
      },
      "source": [
        "batch, labels, negz = generate_batch(batch_size=8, num_skips=2, skip_window=1, negRate = 4)\n",
        "for i in range(8):\n",
        "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n",
        "        reverse_dictionary[labels[i, 0]],\n",
        "        '->', negz[i],  reverse_dictionary[negz[i][0]] )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3081 originated -> 12 as -> [2033 2579  505 2175] contract\n",
            "3081 originated -> 5234 anarchism -> [3318 3818 1729  326] keys\n",
            "12 as -> 3081 originated -> [4832 1771 2300 3135] fate\n",
            "12 as -> 6 a -> [2471  744 4411 4372] husband\n",
            "6 a -> 12 as -> [4487 1409 3892 2625] noise\n",
            "6 a -> 195 term -> [4249   64 3954 1670] mineral\n",
            "195 term -> 2 of -> [3594 4633 2489 4398] developments\n",
            "195 term -> 6 a -> [ 194 4642 4625  655] international\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0QFlzLA-9kQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class SkipGramModel(nn.Module):\n",
        "    \"\"\"Skip gram model of word2vec.\n",
        "    Attributes:\n",
        "        emb_size: Embedding size.\n",
        "        emb_dimention: Embedding dimention, typically from 50 to 500.\n",
        "        u_embedding: Embedding for center word.\n",
        "        v_embedding: Embedding for neibor words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emb_size, emb_dimension):\n",
        "        \"\"\"Initialize model parameters.\n",
        "        Apply for two embedding layers.\n",
        "        Initialize layer weight\n",
        "        Args:\n",
        "            emb_size: Embedding size.\n",
        "            emb_dimention: Embedding dimention, typically from 50 to 500.\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
        "        self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
        "        self.init_emb()\n",
        "\n",
        "    def init_emb(self):\n",
        "        \"\"\"Initialize embedding weight like word2vec.\n",
        "        The u_embedding is a uniform distribution in [-0.5/em_size, 0.5/emb_size], and the elements of v_embedding are zeroes.\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        initrange = 0.5 / self.emb_dimension\n",
        "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.v_embeddings.weight.data.normal_(mean=0, std=initrange)\n",
        "        # self.v_embeddings.weight.data.uniform_(-0, 0)\n",
        "        # self.v_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        \"\"\"Forward process.\n",
        "        As pytorch designed, all variables must be batch format, so all input of this method is a list of word id.\n",
        "        Args:\n",
        "            pos_u: list of center word ids for positive word pairs.\n",
        "            pos_v: list of neibor word ids for positive word pairs.\n",
        "            neg_u: list of center word ids for negative word pairs.\n",
        "            neg_v: list of neibor word ids for negative word pairs.\n",
        "        Returns:\n",
        "            Loss of this process, a pytorch variable.\n",
        "        \"\"\"\n",
        "        emb_u = self.u_embeddings(pos_u)\n",
        "        emb_v = self.v_embeddings(pos_v)\n",
        "        score = torch.mul(emb_u, emb_v).squeeze()\n",
        "        score = torch.sum(score, dim=1)\n",
        "        score = F.logsigmoid(score)\n",
        "\n",
        "        neg_emb_v = self.v_embeddings(neg_v)\n",
        "        neg_score = torch.bmm(neg_emb_v, emb_u.unsqueeze(2)).squeeze()\n",
        "        neg_score = F.logsigmoid(-1 * neg_score)\n",
        "        return -1 * (torch.sum(score)+torch.sum(neg_score))\n",
        "\n",
        "\n",
        "    def save_embedding(self, id2word, file_name, use_cuda):\n",
        "        \"\"\"Save all embeddings to file.\n",
        "        As this class only record word id, so the map from id to word has to be transfered from outside.\n",
        "        Args:\n",
        "            id2word: map from word id to word.\n",
        "            file_name: file name.\n",
        "        Returns:\n",
        "            None.\n",
        "        \"\"\"\n",
        "        if use_cuda:\n",
        "            embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
        "        else:\n",
        "            embedding = self.u_embeddings.weight.data.numpy()\n",
        "        fout = open(file_name, 'w')\n",
        "        fout.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
        "        for wid, w in id2word.items():\n",
        "            e = embedding[wid]\n",
        "            e = ' '.join(map(lambda x: str(x), e))\n",
        "            fout.write('%s %s\\n' % (w, e))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV62RWBsTRnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch, labels, negz = generate_batch(batch_size=8, num_skips=4, skip_window=2, negRate= 4)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfo8uqSEToqv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d0f070a8-7aef-434d-cebc-af48a9fe6f61"
      },
      "source": [
        "print(batch)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3134 3134 3134 3134   46   46   46   46]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThtLA4r0A-EA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "5a0ce218-baa5-45e0-e03a-fc6bcdf2af5c"
      },
      "source": [
        "print(labels)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 195]\n",
            " [  46]\n",
            " [   2]\n",
            " [  59]\n",
            " [3134]\n",
            " [ 156]\n",
            " [  59]\n",
            " [   2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGF2W98zUVE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batchTensor = torch.from_numpy(batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPZsgnX4dIb4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7c62f7b7-a86d-451b-eca8-164143a1f874"
      },
      "source": [
        "batchTensor"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3134, 3134, 3134, 3134,   46,   46,   46,   46], dtype=torch.int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUicXxcSUYlu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "22137745-a5fd-478c-915b-f595a51e46ef"
      },
      "source": [
        "print(batchTensor)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3134, 3134, 3134, 3134,   46,   46,   46,   46], dtype=torch.int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq580dUwdFGa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2bc91003-cb05-4fcd-d0d3-41b2b7aa1c08"
      },
      "source": [
        "print(batchTensor.long())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3134, 3134, 3134, 3134,   46,   46,   46,   46])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU5Y6XI6UjO_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2f6d6f72-11c5-4190-ab18-d4ad46ae09b1"
      },
      "source": [
        "type(batchTensor)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxJpaitATs44",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "b32a5eb2-aa71-4e10-bf1b-85452600d81a"
      },
      "source": [
        "print(labels)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 195]\n",
            " [  46]\n",
            " [   2]\n",
            " [  59]\n",
            " [3134]\n",
            " [ 156]\n",
            " [  59]\n",
            " [   2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTLqg2O5bN0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch, labels, negz = generate_batch(batch_size=8, num_skips=2, skip_window=2, negRate= 4)\n",
        "batchTensor = torch.from_numpy(batch)\n",
        "labels = labels.flatten()\n",
        "LabelTensor = torch.from_numpy(labels)\n",
        "negTensor = torch.from_numpy(negz)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpoK5va7U7EV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = labels.flatten()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q95UZFJeUsVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LabelTensor = torch.from_numpy(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GywaZXr7UuKW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4653a86e-47c7-455a-e808-15331577da3b"
      },
      "source": [
        "print(LabelTensor)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3134,  156,   59,   46,  742,  156,  128,  477], dtype=torch.int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSgiZ_KkUEK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negTensor = torch.from_numpy(negz)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i68vqKeYbBU7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "4274410c-d43c-4311-dff9-ef7bbbab30c8"
      },
      "source": [
        "print(negTensor)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4496,  168, 3551, 2152],\n",
            "        [ 502, 3338,  512, 4771],\n",
            "        [2065, 4546, 2119, 1391],\n",
            "        [4476, 4895, 4344, 2149],\n",
            "        [1253, 4600, 3448,  892],\n",
            "        [ 863, 4854, 2477, 1379],\n",
            "        [4370, 3351, 3106, 4225],\n",
            "        [2331, 2268, 1440, 4517]], dtype=torch.int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35H1wjbrb35q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "919e4fe7-de3f-4918-de89-d69a78d19691"
      },
      "source": [
        "output_file_name = 'theOutpule.file'\n",
        "emb_size = 50000\n",
        "print('embed size is ', emb_size)\n",
        "emb_dimension = 128\n",
        "batch_size = 128\n",
        "window_size = 4\n",
        "iterationsMax = 40000 #200001\n",
        "initial_lr = 1.0\n",
        "skip_gram_model = SkipGramModel(emb_size, emb_dimension)\n",
        "print('got skip gram model')\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('use cuda ? ', use_cuda)\n",
        "if use_cuda:\n",
        "    skip_gram_model.cuda()\n",
        "print('mounted skip gram to cude')\n",
        "optimizer = optim.SGD(\n",
        "    skip_gram_model.parameters(), lr=initial_lr)\n",
        "print('got optimizer')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embed size is  50000\n",
            "got skip gram model\n",
            "use cuda ?  True\n",
            "mounted skip gram to cude\n",
            "got optimizer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU8sDbpn-oLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initial_lr = 1.0\n",
        "optimizer = optim.SGD(\n",
        "    skip_gram_model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0iNnB8csPTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adagrad(\n",
        "    skip_gram_model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPaRqQgFGviY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f4537933-0b27-4067-ba91-c9dbeecfd645"
      },
      "source": [
        "runningLoss = 0\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "for i in range(1000001):\n",
        "    # print('does it even go here', i)\n",
        "    # pos_pairs = dataz.get_batch_pairs(batch_size,\n",
        "    #                                         window_size)\n",
        "    # print('pos pairs is ', pos_pairs)\n",
        "    # neg_v = dataz.get_neg_v_neg_sampling(pos_pairs, 5)\n",
        "    # print('negs are ', neg_v)\n",
        "    # pos_u = [pair[0] for pair in pos_pairs]\n",
        "    # print('pos_u is ', pos_u)\n",
        "    # pos_v = [pair[1] for pair in pos_pairs]\n",
        "    # print('pos_v is ', pos_v)\n",
        "\n",
        "# skip_window = 1  # How many words to consider left and right.\n",
        "# num_skips = 2  # How many times to reuse an input to generate a label.\n",
        "\n",
        "    batch, labels, negz = generate_batch(batch_size=batch_size, num_skips=2, skip_window=1, negRate= 64)\n",
        "    batchTensor = torch.from_numpy(batch)\n",
        "    labels = labels.flatten()\n",
        "    LabelTensor = torch.from_numpy(labels)\n",
        "    negTensor = torch.from_numpy(negz)\n",
        "\n",
        "    pos_u = Variable(torch.LongTensor(batchTensor.long()))\n",
        "    pos_v = Variable(torch.LongTensor(LabelTensor.long()))\n",
        "    neg_v = Variable(torch.LongTensor(negTensor.long()))\n",
        "\n",
        "    if use_cuda:\n",
        "        pos_u = pos_u.cuda()\n",
        "        pos_v = pos_v.cuda()\n",
        "        neg_v = neg_v.cuda()\n",
        "\n",
        "    # optimizer.zero_grad() #set gradients to zero, need to do this at every step or gradients accumulate\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = skip_gram_model.forward(pos_u, pos_v, neg_v)\n",
        "    runningLoss = runningLoss + loss.data.item()\n",
        "    # if i%100 == 0:\n",
        "    #     print('loss is ', loss.data)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    if i%500 == 0:\n",
        "        print('i is ', i)\n",
        "        print('loss is ', runningLoss/500)\n",
        "        runningLoss = 0\n",
        "\n",
        "    # process_bar.set_description(\"Loss: %0.8f, lr: %0.6f\" %\n",
        "    #                             (loss.data[0],\n",
        "    #                                 self.optimizer.param_groups[0]['lr']))\n",
        "    # if i * batch_size % 100000 == 0:\n",
        "    #     lr = self.initial_lr * (1.0 - 1.0 * i / batch_count)\n",
        "    #     for param_group in self.optimizer.param_groups:\n",
        "    #         param_group['lr'] = lr\n",
        "\n",
        "# self.skip_gram_model.save_embedding(\n",
        "#     self.data.id2word, self.output_file_name, self.use_cuda)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i is  0\n",
            "loss is  11.533966796875\n",
            "i is  500\n",
            "loss is  2781.2825708007813\n",
            "i is  1000\n",
            "loss is  1745.5502824707032\n",
            "i is  1500\n",
            "loss is  1458.7799700927735\n",
            "i is  2000\n",
            "loss is  1298.6775208740235\n",
            "i is  2500\n",
            "loss is  1199.1390642700196\n",
            "i is  3000\n",
            "loss is  1106.2075556640625\n",
            "i is  3500\n",
            "loss is  996.426608947754\n",
            "i is  4000\n",
            "loss is  981.5178012695312\n",
            "i is  4500\n",
            "loss is  904.9059776611329\n",
            "i is  5000\n",
            "loss is  838.0480902709961\n",
            "i is  5500\n",
            "loss is  895.2369212646485\n",
            "i is  6000\n",
            "loss is  881.9654464111328\n",
            "i is  6500\n",
            "loss is  784.1718362426758\n",
            "i is  7000\n",
            "loss is  812.1385935058594\n",
            "i is  7500\n",
            "loss is  740.9788065185547\n",
            "i is  8000\n",
            "loss is  792.13784375\n",
            "i is  8500\n",
            "loss is  735.7298042602539\n",
            "i is  9000\n",
            "loss is  703.5626795654297\n",
            "i is  9500\n",
            "loss is  707.3482135620117\n",
            "i is  10000\n",
            "loss is  670.7172040710449\n",
            "i is  10500\n",
            "loss is  695.1228662109374\n",
            "i is  11000\n",
            "loss is  689.4463616333007\n",
            "i is  11500\n",
            "loss is  636.4275872802734\n",
            "i is  12000\n",
            "loss is  640.1980825195312\n",
            "i is  12500\n",
            "loss is  654.956731628418\n",
            "i is  13000\n",
            "loss is  651.9694666137696\n",
            "i is  13500\n",
            "loss is  637.8750974731445\n",
            "i is  14000\n",
            "loss is  615.4714970703125\n",
            "i is  14500\n",
            "loss is  589.9603731994629\n",
            "i is  15000\n",
            "loss is  581.5602883911133\n",
            "i is  15500\n",
            "loss is  615.9471779785156\n",
            "i is  16000\n",
            "loss is  553.0639098205567\n",
            "i is  16500\n",
            "loss is  547.0371248474121\n",
            "i is  17000\n",
            "loss is  524.3282258911133\n",
            "i is  17500\n",
            "loss is  553.2058302001954\n",
            "i is  18000\n",
            "loss is  538.2184163818359\n",
            "i is  18500\n",
            "loss is  555.8986516113281\n",
            "i is  19000\n",
            "loss is  565.9875962524414\n",
            "i is  19500\n",
            "loss is  569.7455681457519\n",
            "i is  20000\n",
            "loss is  550.7248327026367\n",
            "i is  20500\n",
            "loss is  508.03619128417967\n",
            "i is  21000\n",
            "loss is  534.0321705932618\n",
            "i is  21500\n",
            "loss is  524.7760719604493\n",
            "i is  22000\n",
            "loss is  503.5489235229492\n",
            "i is  22500\n",
            "loss is  533.16534765625\n",
            "i is  23000\n",
            "loss is  520.8101039733887\n",
            "i is  23500\n",
            "loss is  517.618163635254\n",
            "i is  24000\n",
            "loss is  517.6606005249023\n",
            "i is  24500\n",
            "loss is  551.920599609375\n",
            "i is  25000\n",
            "loss is  516.3968880310059\n",
            "i is  25500\n",
            "loss is  524.4360505371094\n",
            "i is  26000\n",
            "loss is  510.79114855957033\n",
            "i is  26500\n",
            "loss is  511.4649400024414\n",
            "i is  27000\n",
            "loss is  510.02154888916016\n",
            "i is  27500\n",
            "loss is  523.6396514587402\n",
            "i is  28000\n",
            "loss is  495.02352703857423\n",
            "i is  28500\n",
            "loss is  468.69466864013674\n",
            "i is  29000\n",
            "loss is  466.04032763671876\n",
            "i is  29500\n",
            "loss is  480.6722208557129\n",
            "i is  30000\n",
            "loss is  507.3080696105957\n",
            "i is  30500\n",
            "loss is  510.65023297119143\n",
            "i is  31000\n",
            "loss is  499.67470219421386\n",
            "i is  31500\n",
            "loss is  492.1508430480957\n",
            "i is  32000\n",
            "loss is  484.13038568115235\n",
            "i is  32500\n",
            "loss is  500.2865688171387\n",
            "i is  33000\n",
            "loss is  460.10401965332034\n",
            "i is  33500\n",
            "loss is  476.63524282836914\n",
            "i is  34000\n",
            "loss is  481.3552350769043\n",
            "i is  34500\n",
            "loss is  488.9834962158203\n",
            "i is  35000\n",
            "loss is  481.1027206420898\n",
            "i is  35500\n",
            "loss is  489.7024737854004\n",
            "i is  36000\n",
            "loss is  488.06751275634764\n",
            "i is  36500\n",
            "loss is  484.6753676147461\n",
            "i is  37000\n",
            "loss is  483.04385748291014\n",
            "i is  37500\n",
            "loss is  459.33443634033205\n",
            "i is  38000\n",
            "loss is  443.9949729614258\n",
            "i is  38500\n",
            "loss is  426.79817678833007\n",
            "i is  39000\n",
            "loss is  447.8658403015137\n",
            "i is  39500\n",
            "loss is  463.8095955810547\n",
            "i is  40000\n",
            "loss is  468.4404026184082\n",
            "i is  40500\n",
            "loss is  444.5985641479492\n",
            "i is  41000\n",
            "loss is  465.0464351196289\n",
            "i is  41500\n",
            "loss is  436.10589453125\n",
            "i is  42000\n",
            "loss is  473.06600390625\n",
            "i is  42500\n",
            "loss is  466.78340118408204\n",
            "i is  43000\n",
            "loss is  437.5645834350586\n",
            "i is  43500\n",
            "loss is  444.0456833190918\n",
            "i is  44000\n",
            "loss is  435.84222061157226\n",
            "i is  44500\n",
            "loss is  448.480205078125\n",
            "i is  45000\n",
            "loss is  450.5549094848633\n",
            "i is  45500\n",
            "loss is  467.8177442932129\n",
            "i is  46000\n",
            "loss is  427.8418830871582\n",
            "i is  46500\n",
            "loss is  440.21072192382815\n",
            "i is  47000\n",
            "loss is  445.3950757751465\n",
            "i is  47500\n",
            "loss is  452.5574594421387\n",
            "i is  48000\n",
            "loss is  447.45166162109376\n",
            "i is  48500\n",
            "loss is  424.46517419433593\n",
            "i is  49000\n",
            "loss is  435.95522857666015\n",
            "i is  49500\n",
            "loss is  434.3942958984375\n",
            "i is  50000\n",
            "loss is  411.8484641876221\n",
            "i is  50500\n",
            "loss is  416.7282800445557\n",
            "i is  51000\n",
            "loss is  462.620515838623\n",
            "i is  51500\n",
            "loss is  420.2232905883789\n",
            "i is  52000\n",
            "loss is  423.0401138305664\n",
            "i is  52500\n",
            "loss is  431.25745822143557\n",
            "i is  53000\n",
            "loss is  433.0470596008301\n",
            "i is  53500\n",
            "loss is  435.0924655761719\n",
            "i is  54000\n",
            "loss is  419.3628784790039\n",
            "i is  54500\n",
            "loss is  442.3849535522461\n",
            "i is  55000\n",
            "loss is  436.8826198425293\n",
            "i is  55500\n",
            "loss is  432.58451174926756\n",
            "i is  56000\n",
            "loss is  435.7873816833496\n",
            "i is  56500\n",
            "loss is  438.936498626709\n",
            "i is  57000\n",
            "loss is  428.1524637145996\n",
            "i is  57500\n",
            "loss is  419.8276139984131\n",
            "i is  58000\n",
            "loss is  424.6734534912109\n",
            "i is  58500\n",
            "loss is  416.02608389282227\n",
            "i is  59000\n",
            "loss is  413.4821881866455\n",
            "i is  59500\n",
            "loss is  419.96957681274415\n",
            "i is  60000\n",
            "loss is  426.2429575653076\n",
            "i is  60500\n",
            "loss is  454.34989001464845\n",
            "i is  61000\n",
            "loss is  438.2571311950684\n",
            "i is  61500\n",
            "loss is  439.5586410827637\n",
            "i is  62000\n",
            "loss is  426.8960798950195\n",
            "i is  62500\n",
            "loss is  443.2585037536621\n",
            "i is  63000\n",
            "loss is  433.6340557861328\n",
            "i is  63500\n",
            "loss is  424.91301736450197\n",
            "i is  64000\n",
            "loss is  422.9402601165771\n",
            "i is  64500\n",
            "loss is  408.7732461242676\n",
            "i is  65000\n",
            "loss is  408.42968740844725\n",
            "i is  65500\n",
            "loss is  405.53889627075193\n",
            "i is  66000\n",
            "loss is  398.2738326721191\n",
            "i is  66500\n",
            "loss is  420.4310040588379\n",
            "i is  67000\n",
            "loss is  422.4627400512695\n",
            "i is  67500\n",
            "loss is  440.7176198120117\n",
            "i is  68000\n",
            "loss is  425.6994985961914\n",
            "i is  68500\n",
            "loss is  424.41472528076173\n",
            "i is  69000\n",
            "loss is  419.15007937622073\n",
            "i is  69500\n",
            "loss is  411.38944577026365\n",
            "i is  70000\n",
            "loss is  414.2759348754883\n",
            "i is  70500\n",
            "loss is  408.4526682434082\n",
            "i is  71000\n",
            "loss is  417.7144762878418\n",
            "i is  71500\n",
            "loss is  415.6669436035156\n",
            "i is  72000\n",
            "loss is  394.43371615600586\n",
            "i is  72500\n",
            "loss is  409.40781842041014\n",
            "i is  73000\n",
            "loss is  427.93871569824216\n",
            "i is  73500\n",
            "loss is  422.01619021606444\n",
            "i is  74000\n",
            "loss is  412.0815058288574\n",
            "i is  74500\n",
            "loss is  406.61319036865234\n",
            "i is  75000\n",
            "loss is  403.32981335449216\n",
            "i is  75500\n",
            "loss is  397.2451853942871\n",
            "i is  76000\n",
            "loss is  411.4803069458008\n",
            "i is  76500\n",
            "loss is  414.3939190368652\n",
            "i is  77000\n",
            "loss is  410.33061016845704\n",
            "i is  77500\n",
            "loss is  410.76691772460936\n",
            "i is  78000\n",
            "loss is  415.99037365722654\n",
            "i is  78500\n",
            "loss is  447.0631402587891\n",
            "i is  79000\n",
            "loss is  430.29690951538083\n",
            "i is  79500\n",
            "loss is  399.9155824279785\n",
            "i is  80000\n",
            "loss is  402.89222396850585\n",
            "i is  80500\n",
            "loss is  408.8979753112793\n",
            "i is  81000\n",
            "loss is  396.6604216308594\n",
            "i is  81500\n",
            "loss is  411.6384270019531\n",
            "i is  82000\n",
            "loss is  411.3762538146973\n",
            "i is  82500\n",
            "loss is  400.82815048217776\n",
            "i is  83000\n",
            "loss is  412.54020697021485\n",
            "i is  83500\n",
            "loss is  402.05410995483396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-3c8a1e43c82d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# num_skips = 2  # How many times to reuse an input to generate a label.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_skips\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegRate\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mbatchTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f24dcf4868a0>\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(batch_size, num_skips, skip_window, negRate)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mij\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnegRate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mnumCan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnumCan\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexcludez\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnumCan\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnegs\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_skips\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mnegs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_skips\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mij\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumCan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kfgxdqJL8q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss.data.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSnoJ2NJfp0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
        "#     normalized_embeddings = embeddings / norm\n",
        "#     valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
        "#                                               valid_dataset)\n",
        "#     similarity = tf.matmul(\n",
        "#         valid_embeddings, normalized_embeddings, transpose_b=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBHfTAfPhArd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "squardd = torch.pow(skip_gram_model.u_embeddings.weight.data, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oluxUKh0hb1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(squardd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g549RkFAi9_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sim = torch.mul(skip_gram_model.u_embeddings.weight.data, skip_gram_model.v_embeddings.weight.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-m305KDifOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedNorm = torch.norm(skip_gram_model.u_embeddings.weight.data, p=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA5Gh63kln-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "49fadcf4-17fd-4963-ee90-be3ebc4d049d"
      },
      "source": [
        "print(embedNorm)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(96.4272, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjgG56ZalOsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalizedWeights = torch.div(skip_gram_model.u_embeddings.weight.data, embedNorm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBM-BU4FjSv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validexamples = Variable(torch.LongTensor([ 4, 23, 45, 28, 29, 12, 11, 16, 20, 24, 28, 32, 34, 82, 94, 39, 54, 21, 43, 47, 52, 56, 60, 66, 73, 77, 82, 88, 99, 97])) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1RxQVHimuYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " validExampleTensors = skip_gram_model.u_embeddings( validexamples.cuda() )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z29Bi0XsmOux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalizedValid = torch.div(validExampleTensors, embedNorm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXfvfqMiltdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "finalSim = torch.mm(normalizedValid, normalizedWeights.t() )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpl3QarbCA0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "af997a88-95b9-4365-c37b-0bac6728f2c8"
      },
      "source": [
        "print(finalSim)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 7.1871e-05,  8.0662e-05,  3.2188e-05,  ..., -1.1274e-07,\n",
            "          2.2949e-07, -3.3377e-07],\n",
            "        [ 6.0831e-05,  2.7576e-06,  4.2986e-05,  ..., -4.0243e-08,\n",
            "          1.1123e-07, -5.2457e-07],\n",
            "        [ 5.6719e-05,  1.7365e-04,  7.0849e-05,  ...,  4.2774e-08,\n",
            "         -2.5164e-07,  1.5459e-07],\n",
            "        ...,\n",
            "        [ 5.4501e-05,  1.3877e-05,  6.2630e-05,  ...,  3.5167e-08,\n",
            "         -2.1699e-07, -1.1320e-08],\n",
            "        [ 5.8752e-05,  5.6476e-05,  1.0551e-04,  ..., -3.3040e-09,\n",
            "         -2.6968e-07, -2.6026e-08],\n",
            "        [ 4.1739e-05,  1.4736e-05,  4.6135e-05,  ...,  1.5004e-07,\n",
            "         -1.7628e-07, -3.1063e-08]], device='cuda:0', grad_fn=<MmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZtF8JRhpC70",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "66c70d68-9508-4932-b804-d0b9d5d7ae98"
      },
      "source": [
        "print(finalSim[1].size())"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([50000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-MdVScGnNCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "finalSim.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfnjcgG9jddT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simNumpy = finalSim.cpu().detach().numpy() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkLayUHjndYl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "bde00bd2-2389-45d8-ec02-657c19838bcb"
      },
      "source": [
        "print(simNumpy)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 7.18711599e-05  8.06619064e-05  3.21883235e-05 ... -1.12742200e-07\n",
            "   2.29486503e-07 -3.33770345e-07]\n",
            " [ 6.08308364e-05  2.75757816e-06  4.29855609e-05 ... -4.02430906e-08\n",
            "   1.11230314e-07 -5.24570225e-07]\n",
            " [ 5.67194002e-05  1.73650129e-04  7.08488369e-05 ...  4.27737383e-08\n",
            "  -2.51640017e-07  1.54590197e-07]\n",
            " ...\n",
            " [ 5.45012663e-05  1.38767919e-05  6.26304100e-05 ...  3.51672966e-08\n",
            "  -2.16990472e-07 -1.13196634e-08]\n",
            " [ 5.87524701e-05  5.64756629e-05  1.05511950e-04 ... -3.30402661e-09\n",
            "  -2.69675041e-07 -2.60259139e-08]\n",
            " [ 4.17387564e-05  1.47359297e-05  4.61345116e-05 ...  1.50041501e-07\n",
            "  -1.76283464e-07 -3.10632586e-08]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iku2WiYzpUR5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d14d327c-4ac1-4c3a-8149-cd4df4b29ce4"
      },
      "source": [
        "simNumpy[1].shape"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6axnYERj0Yt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "990ab61f-7fef-4adf-88eb-f80a1764470c"
      },
      "source": [
        "valid_size = [ 4, 23, 45, 28, 29, 12, 11, 16, 20, 24, 28, 32, 34, 82, 94, 39, 54, 21, 43, 47, 52, 56, 60, 66, 73, 77, 82, 88, 99, 97]\n",
        "\n",
        "for i in range(len(valid_size)):\n",
        "    valid_word = reverse_dictionary[valid_size[i]]\n",
        "    top_k = 80  # number of nearest neighbors\n",
        "    nearest = (-simNumpy[i, :]).argsort()[1:top_k + 1]\n",
        "    log_str = 'Nearest to %s:' % valid_word\n",
        "    for k in xrange(top_k):\n",
        "        close_word = reverse_dictionary[nearest[k]]\n",
        "        log_str = '%s %s,' % (log_str, close_word)\n",
        "    print(log_str)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nearest to one: eight, seven, one, zero, six, five, four, three, two, th, century, years, anarchism, km, isbn, c, december, anarchist, million, october, april, m, january, b, anarchists, february, november, september, august, july, march, autism, s, lincoln, late, rand, june, d, f, bc, births, n, p, about, g, feet, autistic, e, deaths, aristotle, x, mid, apollo, pp, miles, female, mi, male, kings, billion, taggart, approximately, links, external, addition, h, early, least, est, ft, days, u, months, l, rest, achilles, lack, year, rearden, ayn,\n",
            "Nearest to seven: zero, eight, seven, six, five, four, three, one, two, th, century, years, anarchism, km, isbn, c, million, anarchist, december, m, b, october, april, january, anarchists, february, november, august, september, july, march, autism, d, lincoln, f, late, rand, s, june, bc, births, about, n, p, e, feet, g, x, deaths, autistic, aristotle, pp, mid, miles, apollo, female, male, mi, billion, external, u, approximately, kings, links, taggart, h, est, early, ft, days, achilles, months, l, lack, rest, rearden, ayn, until, addition, hours,\n",
            "Nearest to its: anarchism, anarchists, anarchist, a, his, its, autism, states, lincoln, rand, their, this, addition, aristotle, autistic, rest, an, order, well, lack, kingdom, apollo, such, taggart, known, part, variety, example, achilles, able, number, ability, result, rearden, ayn, time, opposed, refer, concept, existence, fact, some, based, beginning, of, combination, anchorage, alaska, return, atlas, atheism, idea, europe, contrast, gore, many, instance, moon, used, use, paris, world, all, daughter, algeria, basis, agave, battle, presence, lot, end, dagny, any, favor, absence, amount, angola, majority, slavery, destruction,\n",
            "Nearest to from: in, to, for, by, from, with, anarchism, on, and, is, as, nine, anarchists, are, at, anarchist, was, autism, lincoln, rand, were, into, or, during, united, own, that, s, after, first, eight, zero, autistic, aristotle, including, between, against, rest, where, apollo, although, same, taggart, under, has, through, seven, while, achilles, new, lack, when, since, rearden, ayn, anchorage, but, became, de, atheism, alaska, daughter, atlas, moon, gore, over, late, five, can, result, algeria, dagny, variety, ability, be, agave, however, addition, than, lot,\n",
            "Nearest to or: anarchism, in, anarchists, anarchist, to, nine, and, autism, for, lincoln, rand, by, from, with, on, aristotle, autistic, as, UNK, at, or, rest, apollo, taggart, is, the, lack, achilles, addition, eight, s, rearden, ayn, anchorage, atheism, variety, alaska, result, gore, atlas, are, moon, ability, daughter, algeria, existence, dagny, concept, agave, zero, during, seven, combination, de, lot, late, angola, idea, first, absence, shrugged, refer, november, january, september, was, paris, presence, beginning, slavery, alchemy, same, province, anarchy, favor, return, son, kind, austria, february,\n",
            "Nearest to as: be, is, in, are, to, by, of, for, was, also, that, with, have, anarchism, been, were, on, has, not, anarchists, from, had, can, anarchist, into, but, at, became, autism, lincoln, rand, would, often, does, if, well, and, known, own, where, during, will, when, although, however, after, did, called, they, considered, should, who, autistic, result, sometimes, may, aristotle, now, first, commonly, large, could, see, very, though, against, rest, while, or, so, before, through, apollo, including, taggart, variety, make, most, lack, achilles,\n",
            "Nearest to is: are, was, be, as, has, to, have, were, in, also, that, by, for, had, can, with, of, anarchism, but, on, been, does, from, anarchists, would, see, not, did, will, anarchist, although, if, became, at, where, into, called, may, often, when, however, autism, rand, lincoln, could, and, should, during, considered, they, though, do, so, sometimes, after, own, because, while, commonly, make, aristotle, united, autistic, he, than, result, now, must, against, made, before, rest, through, fact, since, might, taggart, who, able, apollo,\n",
            "Nearest to five: zero, eight, seven, six, five, four, three, two, one, th, century, years, anarchism, km, isbn, c, million, anarchist, december, m, october, b, april, january, anarchists, february, november, august, september, july, autism, march, d, lincoln, s, f, rand, late, june, bc, births, about, n, p, feet, e, g, x, deaths, autistic, aristotle, pp, mid, miles, apollo, female, male, mi, u, billion, external, links, approximately, kings, taggart, h, est, early, ft, days, achilles, months, lack, l, rest, rearden, ayn, addition, hours, until,\n",
            "Nearest to that: to, is, as, that, are, also, not, was, been, anarchism, have, can, anarchists, has, for, were, it, by, had, anarchist, would, they, in, of, with, but, autism, lincoln, rand, often, will, does, he, there, called, considered, on, which, may, from, known, did, should, used, well, could, who, so, became, sometimes, if, see, able, aristotle, autistic, into, however, result, although, rest, made, now, where, use, found, make, refer, at, addition, do, taggart, lack, during, most, said, apollo, fact, commonly, variety, believed,\n",
            "Nearest to with: in, to, for, as, by, is, with, anarchism, from, on, are, and, was, anarchists, at, anarchist, were, that, autism, lincoln, rand, nine, into, or, be, during, has, own, united, after, first, can, autistic, aristotle, where, although, including, but, against, have, rest, between, became, when, same, apollo, through, taggart, while, under, s, had, new, lack, achilles, however, rearden, since, ayn, if, result, will, would, anchorage, de, variety, atheism, alaska, daughter, moon, gore, atlas, ability, within, algeria, large, before, dagny, than, lot,\n",
            "Nearest to from: in, to, for, by, from, with, anarchism, on, and, is, as, nine, anarchists, are, at, anarchist, was, autism, lincoln, rand, were, into, or, during, united, own, that, s, after, first, eight, zero, autistic, aristotle, including, between, against, rest, where, apollo, although, same, taggart, under, has, through, seven, while, achilles, new, lack, when, since, rearden, ayn, anchorage, but, became, de, atheism, alaska, daughter, atlas, moon, gore, over, late, five, can, result, algeria, dagny, variety, ability, be, agave, however, addition, than, lot,\n",
            "Nearest to be: not, to, also, been, have, as, is, are, can, was, that, they, it, has, had, would, were, anarchism, there, he, often, anarchists, does, may, will, used, but, considered, did, should, could, anarchist, known, see, which, sometimes, called, by, who, well, able, do, made, lincoln, autism, rand, so, if, found, only, now, make, become, must, became, more, refer, said, still, referred, usually, commonly, might, generally, with, use, seen, what, result, believed, on, although, however, when, very, aristotle, autistic, into, due, regarded,\n",
            "Nearest to which: anarchists, not, be, anarchist, also, it, been, autism, lincoln, rand, there, known, well, used, they, that, have, able, aristotle, autistic, can, he, addition, considered, rest, which, to, result, refer, apollo, lack, taggart, achilles, often, variety, order, found, such, has, said, would, ability, opposed, believed, fact, ayn, rearden, called, seen, had, a, use, made, idea, return, anchorage, existence, concept, atheism, combination, alaska, gore, atlas, regarded, referred, lot, example, is, moon, daughter, agave, unable, algeria, does, dagny, so, contrast, what, are, sometimes,\n",
            "Nearest to states: anarchism, anarchists, states, anarchist, his, autism, its, lincoln, rand, a, their, this, addition, kingdom, order, aristotle, autistic, rest, example, well, known, lack, apollo, taggart, part, an, able, based, variety, achilles, europe, refer, opposed, time, result, number, ability, used, rearden, ayn, fact, concept, existence, beginning, return, instance, combination, contrast, anchorage, atlas, atheism, alaska, such, idea, paris, gore, all, world, moon, least, battle, algeria, january, basis, daughter, lot, agave, many, some, presence, november, favor, dagny, end, september, absence, use, war, slavery, unable,\n",
            "Nearest to state: anarchists, anarchist, autism, lincoln, rand, of, the, aristotle, autistic, rest, addition, apollo, lack, taggart, achilles, result, variety, ayn, rearden, able, ability, refer, order, anchorage, existence, concept, atheism, alaska, combination, atlas, gore, idea, moon, daughter, fact, algeria, lot, agave, dagny, return, opposed, beginning, absence, presence, paris, angola, contrast, number, shrugged, kind, favor, slavery, basis, introduction, unable, province, anarchy, son, destruction, afghanistan, alchemy, instance, austria, principle, ensure, respect, believed, einstein, establishment, course, origin, belief, galt, asteroids, athens, use, prevent, founder, well, majority,\n",
            "Nearest to have: have, is, also, are, been, has, not, was, as, had, anarchism, can, that, to, anarchists, were, they, would, it, anarchist, does, he, but, there, often, did, used, autism, may, considered, lincoln, rand, will, see, should, called, could, known, by, able, if, sometimes, do, made, became, which, so, well, found, aristotle, who, refer, autistic, result, said, make, now, believed, with, only, become, referred, when, addition, although, rest, more, apollo, lack, however, where, taggart, still, must, commonly, ability, achilles, usually, fact, though,\n",
            "Nearest to been: be, to, not, also, can, have, it, they, as, would, anarchism, that, is, has, had, he, anarchists, was, used, are, there, known, well, may, often, will, considered, anarchist, does, should, which, autism, could, lincoln, were, rand, able, did, made, found, who, sometimes, but, called, seen, more, so, such, only, must, said, now, become, still, do, refer, what, no, referred, aristotle, believed, very, usually, autistic, result, a, might, regarded, make, addition, opposed, if, see, generally, rest, commonly, use, led, apollo, lack,\n",
            "Nearest to four: zero, eight, seven, six, five, four, three, one, two, th, century, years, anarchism, km, isbn, c, million, anarchist, december, m, october, b, april, january, anarchists, february, november, august, september, july, march, autism, d, lincoln, f, rand, late, june, s, bc, births, about, n, p, feet, e, g, x, deaths, autistic, aristotle, pp, miles, mid, apollo, female, male, mi, u, billion, external, links, approximately, kings, taggart, h, est, ft, early, days, months, achilles, lack, rest, rearden, ayn, l, addition, hours, year,\n",
            "Nearest to other: of, anarchists, anarchist, autism, lincoln, rand, the, autistic, aristotle, rest, addition, apollo, lack, taggart, achilles, variety, ayn, rearden, in, result, nine, ability, anchorage, atheism, alaska, existence, concept, atlas, gore, combination, moon, daughter, refer, algeria, order, UNK, lot, agave, dagny, idea, and, beginning, for, absence, presence, paris, return, angola, able, shrugged, november, january, fact, slavery, september, favor, contrast, opposed, kind, basis, province, son, introduction, alchemy, number, february, afghanistan, destruction, austria, anarchy, late, einstein, establishment, galt, principle, unable, asteroids, founder, athens, origin,\n",
            "Nearest to they: not, also, been, anarchism, it, anarchists, to, have, they, there, can, anarchist, that, used, he, known, are, well, autism, would, has, lincoln, had, rand, is, considered, often, was, as, which, able, may, does, will, found, called, made, sometimes, aristotle, did, autistic, should, addition, refer, said, seen, were, could, so, result, rest, but, who, believed, what, referred, lack, apollo, use, such, taggart, opposed, now, regarded, more, do, ability, fact, achilles, variety, only, become, make, order, still, return, ayn, rearden, example, idea,\n",
            "Nearest to most: anarchists, anarchist, autism, lincoln, rand, aristotle, autistic, addition, rest, well, as, known, lack, of, apollo, able, taggart, result, the, also, variety, achilles, refer, order, ability, used, that, not, ayn, rearden, be, to, fact, use, concept, anchorage, idea, existence, a, combination, opposed, atheism, alaska, return, atlas, gore, number, daughter, lot, moon, such, algeria, agave, dagny, it, most, beginning, is, believed, contrast, presence, absence, this, kind, angola, considered, basis, paris, shrugged, unable, favor, introduction, said, belief, slavery, instance, anarchy, respect, province, there,\n",
            "Nearest to many: anarchists, anarchist, autism, lincoln, rand, the, aristotle, autistic, addition, rest, lack, apollo, taggart, a, order, able, well, achilles, result, variety, refer, ayn, rearden, ability, anchorage, existence, known, concept, opposed, atheism, combination, alaska, atlas, fact, gore, idea, return, moon, daughter, lot, algeria, agave, beginning, his, dagny, contrast, number, its, paris, presence, absence, used, states, favor, angola, instance, basis, shrugged, unable, slavery, part, example, kind, introduction, this, believed, province, destruction, afghanistan, anarchy, use, austria, alchemy, respect, son, november, principle, ensure, such, attempted,\n",
            "Nearest to there: not, be, it, also, anarchists, been, there, anarchist, used, known, they, well, autism, lincoln, rand, he, have, considered, able, which, that, addition, aristotle, autistic, often, found, can, refer, rest, example, seen, said, had, result, made, such, believed, lack, order, apollo, called, what, opposed, taggart, use, has, does, variety, referred, would, achilles, so, fact, ability, sometimes, regarded, return, ayn, rearden, idea, existence, combination, unable, anchorage, concept, atheism, led, part, alaska, gore, god, did, contrast, lot, atlas, instance, difficult, more, agave, daughter,\n",
            "Nearest to these: anarchists, anarchist, autism, lincoln, rand, autistic, aristotle, the, addition, rest, apollo, lack, taggart, achilles, result, ayn, variety, rearden, able, refer, order, ability, anchorage, atheism, alaska, existence, concept, atlas, gore, combination, moon, daughter, algeria, lot, agave, idea, dagny, opposed, return, nine, january, fact, beginning, november, september, paris, absence, contrast, presence, angola, favor, february, shrugged, slavery, basis, kind, unable, introduction, province, instance, afghanistan, alchemy, april, destruction, austria, anarchy, march, son, number, principle, einstein, ensure, galt, asteroids, athens, establishment, july, origin, respect, thousands,\n",
            "Nearest to b: zero, eight, anarchism, seven, six, five, four, three, one, two, anarchists, anarchist, th, autism, lincoln, rand, autistic, aristotle, january, december, isbn, april, century, november, february, september, apollo, october, km, rest, addition, taggart, march, july, years, lack, late, august, achilles, c, june, b, ayn, rearden, million, anchorage, atheism, alaska, gore, atlas, ability, variety, refer, moon, result, births, m, dagny, algeria, daughter, existence, agave, lot, combination, concept, mi, paris, absence, mid, angola, feet, shrugged, kings, favor, slavery, beginning, return, presence, able, idea,\n",
            "Nearest to however: anarchists, anarchist, is, autism, lincoln, rand, to, as, are, be, was, of, aristotle, autistic, in, also, for, that, rest, has, addition, apollo, have, taggart, lack, by, result, achilles, with, can, able, been, variety, ayn, refer, rearden, were, not, ability, anchorage, atheism, alaska, had, gore, atlas, existence, concept, moon, idea, daughter, combination, lot, fact, algeria, agave, dagny, from, return, opposed, absence, angola, on, order, kind, shrugged, beginning, presence, paris, slavery, contrast, would, does, unable, believed, favor, introduction, alchemy, province, prevent, basis,\n",
            "Nearest to states: anarchism, anarchists, states, anarchist, his, autism, its, lincoln, rand, a, their, this, addition, kingdom, order, aristotle, autistic, rest, example, well, known, lack, apollo, taggart, part, an, able, based, variety, achilles, europe, refer, opposed, time, result, number, ability, used, rearden, ayn, fact, concept, existence, beginning, return, instance, combination, contrast, anchorage, atlas, atheism, alaska, such, idea, paris, gore, all, world, moon, least, battle, algeria, january, basis, daughter, lot, agave, many, some, presence, november, favor, dagny, end, september, absence, use, war, slavery, unable,\n",
            "Nearest to if: anarchists, anarchist, is, be, autism, lincoln, rand, are, as, was, also, have, to, has, aristotle, autistic, been, that, rest, had, addition, not, apollo, were, taggart, lack, result, can, able, achilles, refer, by, ayn, variety, rearden, ability, for, in, anchorage, does, atheism, with, alaska, gore, atlas, existence, concept, moon, combination, lot, idea, daughter, fact, algeria, agave, dagny, return, would, opposed, nine, considered, but, absence, angola, kind, believed, shrugged, order, paris, unable, presence, slavery, beginning, contrast, did, favor, introduction, prevent, alchemy, on,\n",
            "Nearest to while: anarchists, anarchist, of, autism, lincoln, rand, to, in, is, nine, autistic, aristotle, for, as, rest, apollo, addition, taggart, by, lack, with, are, achilles, from, was, result, ayn, rearden, and, variety, ability, anchorage, atheism, refer, on, alaska, gore, atlas, moon, existence, daughter, concept, combination, able, algeria, lot, dagny, agave, idea, return, absence, angola, fact, shrugged, beginning, presence, paris, slavery, kind, order, opposed, favor, november, contrast, january, september, alchemy, province, introduction, afghanistan, son, basis, austria, that, unable, einstein, destruction, anarchy, asteroids, february,\n",
            "Nearest to will: be, anarchism, can, is, anarchists, also, been, not, anarchist, are, was, would, have, that, autism, lincoln, rand, as, has, will, may, had, it, they, were, used, considered, should, for, able, does, aristotle, autistic, by, often, well, could, rest, known, addition, but, refer, apollo, result, lack, with, taggart, called, did, achilles, found, ability, there, variety, made, ayn, rearden, he, sometimes, said, must, anchorage, fact, opposed, seen, idea, alaska, atheism, believed, concept, return, existence, atlas, gore, so, moon, which, combination, daughter, make,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_ni7pBL_59e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_size = [ 4, 23, 45, 283, 292, 12, 113, 167, 200, 204, 208, 432, 354, 382, 194, 339, 54, 221, 121, 111]\n",
        "\n",
        "for i in valid_size:\n",
        "    valid_word = reverse_dictionary[i]\n",
        "    top_k = 8  # number of nearest neighbors\n",
        "    nearest = (-simNumpy[i, :]).argsort()[1:top_k + 1]\n",
        "    log_str = 'Nearest to %s:' % valid_word\n",
        "    for k in xrange(top_k):\n",
        "        close_word = reverse_dictionary[nearest[k]]\n",
        "        log_str = '%s %s,' % (log_str, close_word)\n",
        "    print(log_str)\n",
        "for i in valid_size:\n",
        "    valid_word = reverse_dictionary[i]\n",
        "    top_k = 8  # number of nearest neighbors\n",
        "    nearest = (-simNumpy[i, :]).argsort()[1:top_k + 1]\n",
        "    log_str = 'Nearest to %s:' % valid_word\n",
        "    for k in xrange(top_k):\n",
        "        close_word = reverse_dictionary[nearest[k]]\n",
        "        log_str = '%s %s,' % (log_str, close_word)\n",
        "    print(log_str)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}