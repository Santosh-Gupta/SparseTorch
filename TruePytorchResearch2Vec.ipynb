{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TruePytorchResearch2Vec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santosh-Gupta/SparseTorch/blob/master/TruePytorchResearch2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZhhRX3R-lMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy\n",
        "import random \n",
        "import hashlib\n",
        "\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "from collections import deque\n",
        "numpy.random.seed(12345)\n",
        "\n",
        "use_cuda = True\n",
        "# if use_cuda and torch.cuda.is_available():\n",
        "#     net.cuda()\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import collections\n",
        "import hashlib\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from tempfile import gettempdir\n",
        "import zipfile\n",
        "\n",
        "data_index = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFwIIQEaleQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "from numpy import genfromtxt\n",
        "\n",
        "vocabulary_size = 1666577 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRR5s_cHCuWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from tempfile import gettempdir\n",
        "import urllib.request\n",
        "from six.moves import xrange\n",
        "import zipfile\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJUukIdJlHGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#link to hold dictionary folder https://drive.google.com/drive/u/0/folders/1q2D8xlzyQmUjggLStsfVEtgq3_GWxcKl\n",
        "#in san gupta ml\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "download_file_from_google_drive('1RHVwT1slwbhPlNTTF1JKS7agc4hQSm5c', 'Data.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwMFVtPtltFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "fbbe0a0a-2787-4579-8fe7-ac6e92b1161b"
      },
      "source": [
        "my_data = np.load('Data.npy', allow_pickle=True)\n",
        "print(my_data[0:15])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 list([])]\n",
            " [1\n",
            "  list([421089, 510776, 6403, 1554618, 1451018, 1448326, 1411539, 734702, 263668, 276186, 374145, 712335, 1540518, 732154, 1256014, 370711])]\n",
            " [2 list([])]\n",
            " [3\n",
            "  list([896236, 552833, 290985, 744122, 660888, 1492583, 324439, 1497464, 906952, 890270, 800459, 656974, 464637, 432398, 672494, 1501784, 1551199, 169770, 880915, 1257202, 1647789, 431318, 167368, 1309706, 645636, 1589247, 952101, 1594224, 566783, 1020670, 1530466, 572983, 393055, 923629, 1349376, 455838, 168364, 1419708, 670762, 64953])]\n",
            " [4\n",
            "  list([377701, 646875, 1527223, 458740, 1022675, 668690, 910689, 951671, 717587, 1655779, 670477, 66465, 374116, 450320, 83567, 863721, 1328431, 1585189, 1439964])]\n",
            " [5\n",
            "  list([1394328, 658435, 1338541, 1024419, 1193128, 1416126, 600891, 1133836, 1502110, 38954, 200361, 1271103, 914246, 580300, 337729, 316423, 1631441, 75283, 153695, 294419, 904711, 234803, 341096, 350848, 344889, 146171, 610828, 475984, 462863, 768574, 1060750, 753854, 355396, 457861, 1159063, 1074007, 919943, 1045192, 550452])]\n",
            " [6 list([1590967])]\n",
            " [7\n",
            "  list([731413, 371576, 101514, 291861, 668641, 812990, 457315, 1428604, 216222, 313539, 475783, 1384755, 1426847, 1612089, 124271, 1259377, 1209643, 994466, 1437081, 300318, 1432000])]\n",
            " [8\n",
            "  list([537201, 743717, 194785, 886957, 877387, 405472, 145841, 662184])]\n",
            " [9\n",
            "  list([727615, 1033127, 1488761, 205826, 1278175, 1406008, 546451, 739509, 1412014, 1628720, 797494, 381440, 738525, 103954, 1293419, 778810, 292339, 906068])]\n",
            " [10\n",
            "  list([767034, 632192, 943392, 1444320, 136613, 891973, 1497365, 1580850, 305850, 1000807, 1449216, 1476570, 301317, 1500249, 1262399, 501012, 1115942, 1058776, 1447436, 1357729, 1592057, 1498628, 1618410, 987861, 1504522])]\n",
            " [11 list([260217, 735748, 576441, 596114])]\n",
            " [12\n",
            "  list([1139960, 983105, 295417, 557677, 1252174, 697297, 881771, 1211343, 863739, 714478, 1219226, 935298, 1043281, 1229931, 839873, 1153430, 1080857, 1654324, 137984, 1025220, 696853, 570840, 1590806, 1351588, 595129, 964004, 1472538, 239877])]\n",
            " [13 list([])]\n",
            " [14\n",
            "  list([1255078, 525049, 1481675, 1231620, 894550, 127476, 384389, 737607, 1651253, 771448, 284807, 1127559, 213372, 687169, 1480889, 1395063, 369818, 1454291])]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ievH_QyNk6Od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_index = 0\n",
        "epoch_index = 0\n",
        "recEpoch_indexA = 0 #Used to help keep store of the total number of epoches with the models\n",
        "\n",
        "def generate_batch(batch_size, inputCount, negRate): #batch size = number of labels\n",
        "  #inputCount = number of inputs per label\n",
        "    global data_index, epoch_index\n",
        "    \n",
        "    batch = np.ndarray(shape=(batch_size, inputCount), dtype=np.int32) \n",
        "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    negs = np.ndarray(shape=(batch_size, negRate), dtype=np.int32)\n",
        "    \n",
        "    n=0\n",
        "    while n < batch_size:\n",
        "      if len(    set(my_data[data_index, 1])   ) >= inputCount:\n",
        "        labels[n,0] = my_data[data_index, 0]\n",
        "        batch[n] = random.sample( set(my_data[data_index, 1]),  inputCount) #gets inputCount number of random inputs\n",
        "        # print('labels[n,0] ',  labels[n,0])\n",
        "        # print('batch[n] ',  batch[n])\n",
        "        # print('labels[n] ',  labels[n])\n",
        "        excludez = np.concatenate([labels[n] , batch[n]])\n",
        "        # negs[n] = random.sample([i for i in range(0, vocabulary_size) if i not in excludez ], 64)\n",
        "        ij=0\n",
        "        while ij < negRate:\n",
        "            i = int(random.random() * vocabulary_size)\n",
        "            if i in excludez or i in negs[n]:\n",
        "                continue\n",
        "            negs[n, ij] = i\n",
        "            ij = ij + 1\n",
        "\n",
        "        n = n+1\n",
        "        data_index = (data_index + 1) % len(my_data) #may have to do something like len my_data[:]\n",
        "        if data_index == 0:\n",
        "          epoch_index = epoch_index + 1\n",
        "          print('Completed %d Epochs' % epoch_index)\n",
        "      else:\n",
        "        data_index = (data_index + 1) % len(my_data)\n",
        "        if data_index == 0:\n",
        "          epoch_index = epoch_index + 1\n",
        "          print('Completed %d Epochs' % epoch_index)\n",
        "\n",
        "    return batch, labels, negs     \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gibQcii9I3RD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "e88aec38-b259-4237-a880-0d2655a97071"
      },
      "source": [
        "here, goes, nothing = generate_batch(5, 4, 16) # to do next, insert %len(headernumber)\n",
        "print('batch', here)\n",
        "print('labels', goes)\n",
        "print('negs', nothing)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch [[ 710144  525974 1118490 1172566]\n",
            " [ 376133 1302964  990165   87552]\n",
            " [1620250  937582   89573  790989]\n",
            " [1021864 1211840 1278123 1509001]\n",
            " [1407057  549764 1578486 1205414]]\n",
            "labels [[30]\n",
            " [31]\n",
            " [32]\n",
            " [33]\n",
            " [34]]\n",
            "negs [[ 608178  849556  106912  262859  293764  967239 1568186  420083 1019243\n",
            "  1570086  462324  643319  973638  934154  964726  132631]\n",
            " [ 741176  483672  660059 1153338  204686  163595  871480  720891  684114\n",
            "   138528   81496 1297941   86847 1202418   94355  645307]\n",
            " [ 671836  478456 1554621 1534952  527760  229759   67100  256847  416427\n",
            "   198374  613843 1257918  678279  461780  472337   31240]\n",
            " [1229705 1653621 1291675 1400285  959024  524165  521770 1133146 1348229\n",
            "   180810  206058 1258395 1469423  474814 1024907 1322163]\n",
            " [1544897  163846  730917 1206408  789527 1454806  732438  593214  804977\n",
            "  1599056  497777 1102774  310016   72580 1531724 1472930]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0QFlzLA-9kQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class SkipGramModel(nn.Module):\n",
        "    \"\"\"Skip gram model of word2vec.\n",
        "    Attributes:\n",
        "        emb_size: Embedding size.\n",
        "        emb_dimention: Embedding dimention, typically from 50 to 500.\n",
        "        u_embedding: Embedding for center word.\n",
        "        v_embedding: Embedding for neibor words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emb_size, emb_dimension):\n",
        "        \"\"\"Initialize model parameters.\n",
        "        Apply for two embedding layers.\n",
        "        Initialize layer weight\n",
        "        Args:\n",
        "            emb_size: Embedding size.\n",
        "            emb_dimention: Embedding dimention, typically from 50 to 500.\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
        "        self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
        "        self.init_emb()\n",
        "\n",
        "    def init_emb(self):\n",
        "        \"\"\"Initialize embedding weight like word2vec.\n",
        "        The u_embedding is a uniform distribution in [-0.5/em_size, 0.5/emb_size], and the elements of v_embedding are zeroes.\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        initrange = 0.5 / self.emb_dimension\n",
        "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
        "        # self.v_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        \"\"\"Forward process.\n",
        "        As pytorch designed, all variables must be batch format, so all input of this method is a list of word id.\n",
        "        Args:\n",
        "            pos_u: list of center word ids for positive word pairs.\n",
        "            pos_v: list of neibor word ids for positive word pairs.\n",
        "            neg_u: list of center word ids for negative word pairs.\n",
        "            neg_v: list of neibor word ids for negative word pairs.\n",
        "        Returns:\n",
        "            Loss of this process, a pytorch variable.\n",
        "        \"\"\"\n",
        "        emb_u = self.u_embeddings(pos_u)\n",
        "        emb_v = self.v_embeddings(pos_v)\n",
        "        score = torch.mul(emb_u, emb_v).squeeze()\n",
        "        score = torch.sum(score, dim=1)\n",
        "        score = F.logsigmoid(score)\n",
        "        neg_emb_v = self.v_embeddings(neg_v)\n",
        "        neg_score = torch.bmm(neg_emb_v, emb_u.unsqueeze(2)).squeeze()\n",
        "        neg_score = F.logsigmoid(-1 * neg_score)\n",
        "        return -1 * (torch.sum(score)+torch.sum(neg_score))\n",
        "\n",
        "    def save_embedding(self, id2word, file_name, use_cuda):\n",
        "        \"\"\"Save all embeddings to file.\n",
        "        As this class only record word id, so the map from id to word has to be transfered from outside.\n",
        "        Args:\n",
        "            id2word: map from word id to word.\n",
        "            file_name: file name.\n",
        "        Returns:\n",
        "            None.\n",
        "        \"\"\"\n",
        "        if use_cuda:\n",
        "            embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
        "        else:\n",
        "            embedding = self.u_embeddings.weight.data.numpy()\n",
        "        fout = open(file_name, 'w')\n",
        "        fout.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
        "        for wid, w in id2word.items():\n",
        "            e = embedding[wid]\n",
        "            e = ' '.join(map(lambda x: str(x), e))\n",
        "            fout.write('%s %s\\n' % (w, e))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPaRqQgFGviY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "output_file_name = 'theOutpule.file'\n",
        "emb_size = 1666577\n",
        "print('embed size is ', emb_size)\n",
        "emb_dimension = 32\n",
        "batch_size = 128\n",
        "window_size = 4\n",
        "iterationsMax = 10000\n",
        "initial_lr = 0.025\n",
        "skip_gram_model = SkipGramModel(emb_size, emb_dimension)\n",
        "print('got skip gram model')\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('use cuda ? ', use_cuda)\n",
        "if use_cuda:\n",
        "    skip_gram_model.cuda()\n",
        "print('mounted skip gram to cude')\n",
        "optimizer = optim.Adagrad(\n",
        "    skip_gram_model.parameters())\n",
        "print('got optimizer')\n",
        "\n",
        "for i in range(iterationsMax):\n",
        "\n",
        "    batch, labels, negz = generate_batch(batch_size=batch_size, num_skips=2, skip_window=1, negRate= 64)\n",
        "    batchTensor = torch.from_numpy(batch)\n",
        "    labels = labels.flatten()\n",
        "    LabelTensor = torch.from_numpy(labels)\n",
        "    negTensor = torch.from_numpy(negz)\n",
        "\n",
        "    pos_u = Variable(torch.LongTensor(batchTensor.long()))\n",
        "    pos_v = Variable(torch.LongTensor(LabelTensor.long()))\n",
        "    neg_v = Variable(torch.LongTensor(negTensor.long()))\n",
        "\n",
        "    if use_cuda:\n",
        "        pos_u = pos_u.cuda()\n",
        "        pos_v = pos_v.cuda()\n",
        "        neg_v = neg_v.cuda()\n",
        "\n",
        "    # optimizer.zero_grad() #set gradients to zero, need to do this at every step or gradients accumulate\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = skip_gram_model.forward(pos_u, pos_v, neg_v)\n",
        "    runningLoss = runningLoss + loss.data.item()\n",
        "    # if i%100 == 0:\n",
        "    #     print('loss is ', loss.data)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    if i%500 == 0:\n",
        "        print('i is ', i)\n",
        "        print('loss is ', runningLoss/500)\n",
        "        runningLoss = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0ySZtFeQNlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedNorm = torch.norm(skip_gram_model.u_embeddings.weight.data, p=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH-R7-CFQOGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalizedWeights = torch.div(skip_gram_model.u_embeddings.weight.data, embedNorm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWt6aI2pQPcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validexamples = Variable(torch.LongTensor([ 4, 23, 45, 28, 29, 12, 11, 16, 20, 24, 28, 32, 34, 82, 94, 39, 54, 21])) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XztOdELtQQyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " validExampleTensors = skip_gram_model.u_embeddings( validexamples.cuda() )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Elzwy6gHQV8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalizedValid = torch.div(validExampleTensors, embedNorm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGCTKHNTQXnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "finalSim = torch.mm(normalizedValid, normalizedWeights.t() )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFQhgu_7QbDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simNumpy = finalSim.cpu().detach().numpy() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la4oM9kTQc-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_size = [ 4, 23, 45, 283, 292, 12, 113, 167, 200, 204, 208, 432, 354, 382, 194, 339, 54, 221]\n",
        "\n",
        "for i in range(len(valid_size)):\n",
        "    valid_word = reverse_dictionary[valid_size[i]]\n",
        "    top_k = 80  # number of nearest neighbors\n",
        "    nearest = (-simNumpy[i, :]).argsort()[1:top_k + 1]\n",
        "    log_str = 'Nearest to %s:' % valid_word\n",
        "    for k in xrange(top_k):\n",
        "        close_word = reverse_dictionary[nearest[k]]\n",
        "        log_str = '%s %s,' % (log_str, close_word)\n",
        "    print(log_str)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}